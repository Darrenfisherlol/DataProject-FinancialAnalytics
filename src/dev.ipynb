{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FMPKey = \"\"\n",
    "\n",
    "# from current filepath \n",
    "\n",
    "import pathlib\n",
    "currentPath = str(pathlib.Path().parent.resolve()) + \"\\\\data.json\"\n",
    "\n",
    "with open(currentPath) as f:\n",
    "    d = json.load(f)\n",
    "    # print(d[\"settings\"])\n",
    "    FMPKey = d[\"settings\"][\"FMP\"][\"fmpkey\"]\n",
    "    # print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Equity:\n",
    "\n",
    "    tickerList = []\n",
    "\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "\n",
    "        # income statement\n",
    "        self.incomestatement = []\n",
    "\n",
    "        #  cash flow\n",
    "        self.cashFlow = []\n",
    "\n",
    "        #  price\n",
    "        self.price = []\n",
    "        \n",
    "        # volume\n",
    "        self.volumeInsider = []\n",
    "        self.gov = []        \n",
    "\n",
    "    def RevGrowthOverTime(self):\n",
    "        print(f\"{self} + income\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ticker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20528\\3522190325.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Price\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgetAllStockPrice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}?apikey={FMPKey}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ticker' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "--\n",
    "-- Get a list of all tickers available to pull\n",
    "--\n",
    "'''\n",
    "\n",
    "# Price\n",
    "getAllStockPrice = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}?apikey={FMPKey}\"\n",
    "\n",
    "\n",
    "#getAllStockPrice = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}?from={2023-08-10}&to={2023-09-10}?apikey={FMPKey}\"\n",
    "\n",
    "\n",
    "# https://financialmodelingprep.com/api/v3/historical-chart/5min/AAPL?from=2023-08-10&to=2023-09-10&apikey=XRMOigyOp7OnWYjcQWR4nH6hoSnTGosL\n",
    "# \n",
    "\n",
    "# list of all possible tickers\n",
    "getAllTickers = f\"https://financialmodelingprep.com/api/v3/stock/list?apikey={FMPKey}\"\n",
    "\n",
    "# Income Statement\n",
    "getAnnualData = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?period={annual}&apikey={FMPKey}\"\n",
    "getQuarterlyData = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?period={quarter}&apikey={FMPKey}\"\n",
    "\n",
    "# Balance Sheet\n",
    "getBalanceSheetData = f\"https://financialmodelingprep.com/api/v3/balance-sheet-statement/{ticker}?period={quarter}&apikey={FMPKey}\"\n",
    "\n",
    "# Cash Flow\n",
    "getCashFlowData = f\"https://financialmodelingprep.com/api/v3/cash-fflow-statement/{ticker}?period={quarter}&apikey={FMPKey}\"\n",
    "\n",
    "# Insider buys & volume \n",
    "# house = \"https://financialmodelingprep.com/api/v4/senate-disclosure?symbol=AAPL&apikey=XRMOigyOp7OnWYjcQWR4nH6hoSnTGosL\"\n",
    "# senate = \"https://financialmodelingprep.com/api/v4/senate-trading?symbol=AAPL&apikey=XRMOigyOp7OnWYjcQWR4nH6hoSnTGosL\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--\n",
    "-- Add new tickers if needed\n",
    "--\n",
    "'''\n",
    "\n",
    "# All Tickers possible\n",
    "allTickers = ()\n",
    "\n",
    "\n",
    "# list of all saved tickers in db\n",
    "TickersSavedData = ()\n",
    "\n",
    "# list of tickers we need to get\n",
    "getTickerData = ()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Click Enter to stop adding new data\")\n",
    "\n",
    "while True:\n",
    "    tickerInput = str(input(\"What ticker do you want to research?\"))\n",
    "\n",
    "    # exists, add or not \n",
    "    if tickerInput in allTickers:\n",
    "\n",
    "        #  if ticker is not in saved list of tickers, add\n",
    "        if tickerInput not in TickersSavedData:\n",
    "            TickersSavedData.append(tickerInput)\n",
    "            \n",
    "            # get data of recently added tickers\n",
    "            getTickerData.append(tickerInput)\n",
    "    if tickerInput is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given list of tickers we need data on, get raw data\n",
    "#getTickerData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockDataSave = {}\n",
    "\n",
    "for ticker in getTickerData:\n",
    "    \n",
    "    getQuarterlyData = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?period=quarter&apikey={FMPKey}\"\n",
    "    stockDataSave[ticker] = getQuarterlyData\n",
    "\n",
    "\n",
    "\n",
    "# List of items we will drop\n",
    "dataNotNeeded = [\"\"]\n",
    "\n",
    "for data in stockDataSave:\n",
    "    stockDataSave[data].pop(dataNotNeeded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTickerData = [\"aapl\",\"msft\"]\n",
    "stockDataSave = {}\n",
    "stockdata = \"\"\n",
    "for ticker in getTickerData:\n",
    "    \n",
    "    getQuarterlyData = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?period=annual&apikey={FMPKey}\"\n",
    "    \n",
    "    # print(getQuarterlyData)\n",
    "    data = requests.get(getQuarterlyData)\n",
    "    # print(data.content)\n",
    "    stockdata = (data)\n",
    "    stockDataSave[ticker] = data.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stockDataSave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2022-09-24',\n",
       " 'symbol': 'AAPL',\n",
       " 'reportedCurrency': 'USD',\n",
       " 'cik': '0000320193',\n",
       " 'fillingDate': '2022-10-28',\n",
       " 'acceptedDate': '2022-10-27 18:01:14',\n",
       " 'calendarYear': '2022',\n",
       " 'period': 'FY',\n",
       " 'revenue': 394328000000,\n",
       " 'costOfRevenue': 223546000000,\n",
       " 'grossProfit': 170782000000,\n",
       " 'grossProfitRatio': 0.4330963056,\n",
       " 'researchAndDevelopmentExpenses': 26251000000,\n",
       " 'generalAndAdministrativeExpenses': 0,\n",
       " 'sellingAndMarketingExpenses': 0,\n",
       " 'sellingGeneralAndAdministrativeExpenses': 25094000000,\n",
       " 'otherExpenses': -334000000,\n",
       " 'operatingExpenses': 51345000000,\n",
       " 'costAndExpenses': 274891000000,\n",
       " 'interestIncome': 2825000000,\n",
       " 'interestExpense': 2931000000,\n",
       " 'depreciationAndAmortization': 11104000000,\n",
       " 'ebitda': 130541000000,\n",
       " 'ebitdaratio': 0.3310467428,\n",
       " 'operatingIncome': 119437000000,\n",
       " 'operatingIncomeRatio': 0.302887444,\n",
       " 'totalOtherIncomeExpensesNet': -334000000,\n",
       " 'incomeBeforeTax': 119103000000,\n",
       " 'incomeBeforeTaxRatio': 0.3020404333,\n",
       " 'incomeTaxExpense': 19300000000,\n",
       " 'netIncome': 99803000000,\n",
       " 'netIncomeRatio': 0.2530964071,\n",
       " 'eps': 6.15,\n",
       " 'epsdiluted': 6.11,\n",
       " 'weightedAverageShsOut': 16215963000,\n",
       " 'weightedAverageShsOutDil': 16325819000,\n",
       " 'link': 'https://www.sec.gov/Archives/edgar/data/320193/000032019322000108/0000320193-22-000108-index.htm',\n",
       " 'finalLink': 'https://www.sec.gov/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop data that we dont want\n",
    "#\n",
    "# reportedCurrency, cik, fillingDate, acceptedDate,calendarYear,period,eps,epsdiluted,weightedAverageShsOut,weightedAverageShsOutDil,finalLink \n",
    "#\n",
    "\n",
    "stockDataSave[\"aapl\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rev growth vs cost growth over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aapl': []}\n",
      "{'aapl': [{'fillingDate': '2023-11-03', 'calendarYear': '2023', 'period': 'FY', 'link': 'https://www.sec.gov/Archives/edgar/data/320193/000032019323000106/0000320193-23-000106-index.htm', 'revenue': 383285000000, 'costOfRevenue': 214137000000, 'grossProfit': 169148000000, 'researchAndDevelopmentExpenses': 29915000000, 'generalAndAdministrativeExpenses': 0, 'sellingAndMarketingExpenses': 0, 'sellingGeneralAndAdministrativeExpenses': 24932000000, 'otherExpenses': -565000000, 'operatingExpenses': 54847000000, 'costAndExpenses': 268984000000, 'interestIncome': 3750000000, 'interestExpense': 3933000000, 'depreciationAndAmortization': 11519000000, 'ebitda': 125820000000, 'operatingIncome': 114301000000}, {'fillingDate': '2022-10-28', 'calendarYear': '2022', 'period': 'FY', 'link': 'https://www.sec.gov/Archives/edgar/data/320193/000032019322000108/0000320193-22-000108-index.htm', 'revenue': 394328000000, 'costOfRevenue': 223546000000, 'grossProfit': 170782000000, 'researchAndDevelopmentExpenses': 26251000000, 'generalAndAdministrativeExpenses': 0, 'sellingAndMarketingExpenses': 0, 'sellingGeneralAndAdministrativeExpenses': 25094000000, 'otherExpenses': -334000000, 'operatingExpenses': 51345000000, 'costAndExpenses': 274891000000, 'interestIncome': 2825000000, 'interestExpense': 2931000000, 'depreciationAndAmortization': 11104000000, 'ebitda': 130541000000, 'operatingIncome': 119437000000}, {'fillingDate': '2021-10-29', 'calendarYear': '2021', 'period': 'FY', 'link': 'https://www.sec.gov/Archives/edgar/data/320193/000032019321000105/0000320193-21-000105-index.htm', 'revenue': 365817000000, 'costOfRevenue': 212981000000, 'grossProfit': 152836000000, 'researchAndDevelopmentExpenses': 21914000000, 'generalAndAdministrativeExpenses': 0, 'sellingAndMarketingExpenses': 0, 'sellingGeneralAndAdministrativeExpenses': 21973000000, 'otherExpenses': 258000000, 'operatingExpenses': 43887000000, 'costAndExpenses': 256868000000, 'interestIncome': 2843000000, 'interestExpense': 2645000000, 'depreciationAndAmortization': 11284000000, 'ebitda': 120233000000, 'operatingIncome': 108949000000}, {'fillingDate': '2020-10-30', 'calendarYear': '2020', 'period': 'FY', 'link': 'https://www.sec.gov/Archives/edgar/data/320193/000032019320000096/0000320193-20-000096-index.htm', 'revenue': 274515000000, 'costOfRevenue': 169559000000, 'grossProfit': 104956000000, 'researchAndDevelopmentExpenses': 18752000000, 'generalAndAdministrativeExpenses': 0, 'sellingAndMarketingExpenses': 0, 'sellingGeneralAndAdministrativeExpenses': 19916000000, 'otherExpenses': 803000000, 'operatingExpenses': 38668000000, 'costAndExpenses': 208227000000, 'interestIncome': 3763000000, 'interestExpense': 2873000000, 'depreciationAndAmortization': 11056000000, 'ebitda': 77344000000, 'operatingIncome': 66288000000}, {'fillingDate': '2019-10-31', 'calendarYear': '2019', 'period': 'FY', 'link': 'https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/0000320193-19-000119-index.htm', 'revenue': 260174000000, 'costOfRevenue': 161782000000, 'grossProfit': 98392000000, 'researchAndDevelopmentExpenses': 16217000000, 'generalAndAdministrativeExpenses': 0, 'sellingAndMarketingExpenses': 0, 'sellingGeneralAndAdministrativeExpenses': 18245000000, 'otherExpenses': 1807000000, 'operatingExpenses': 34462000000, 'costAndExpenses': 196244000000, 'interestIncome': 4961000000, 'interestExpense': 3576000000, 'depreciationAndAmortization': 12547000000, 'ebitda': 76477000000, 'operatingIncome': 63930000000}], 'msft': []}\n"
     ]
    }
   ],
   "source": [
    "''' data want to keep\n",
    "-- symbol\n",
    "-- fillingDate\n",
    "-- calendarYear\n",
    "-- period\n",
    "-- link \n",
    "-- revenue\n",
    "-- costOfRevenue\n",
    "-- grossProfit\n",
    "-- researchAndDevelopmentExpenses\n",
    "-- generalAndAdministrativeExpenses\n",
    "-- sellingAndMarketingExpenses\n",
    "-- sellingGeneralAndAdministrativeExpenses\n",
    "-- otherExpenses\n",
    "-- operatingExpenses\n",
    "-- costAndExpenses\n",
    "-- interestIncome\n",
    "-- interestExpense\n",
    "-- depreciationAndAmortization\n",
    "-- ebitda\n",
    "-- operatingIncome\n",
    "'''\n",
    "cleanIncomeStatement = {}\n",
    "# {'aapl': [  {'date': '2023-09-30'}  ]}\n",
    "\n",
    "for ticker in stockDataSave.keys():\n",
    "    \n",
    "    # ticker = apple\n",
    "    cleanIncomeStatement[ticker] = []\n",
    "\n",
    "    for quarter in stockDataSave[ticker]:\n",
    "        \n",
    "        # print(quarter) dict {} of items\n",
    "        fillingDate = {}\n",
    "\n",
    "        fillingDate[\"fillingDate\"] = quarter[\"fillingDate\"]\n",
    "        fillingDate[\"calendarYear\"] = quarter[\"calendarYear\"]\n",
    "        fillingDate[\"period\"] = quarter[\"period\"]\n",
    "        fillingDate[\"link\"] = quarter[\"link\"]\n",
    "        fillingDate[\"revenue\"] = quarter[\"revenue\"]\n",
    "        fillingDate[\"costOfRevenue\"] = quarter[\"costOfRevenue\"]\n",
    "        fillingDate[\"grossProfit\"] = quarter[\"grossProfit\"]\n",
    "        fillingDate[\"researchAndDevelopmentExpenses\"] = quarter[\"researchAndDevelopmentExpenses\"]\n",
    "        fillingDate[\"generalAndAdministrativeExpenses\"] = quarter[\"generalAndAdministrativeExpenses\"]\n",
    "        fillingDate[\"sellingAndMarketingExpenses\"] = quarter[\"sellingAndMarketingExpenses\"]\n",
    "        fillingDate[\"sellingGeneralAndAdministrativeExpenses\"] = quarter[\"sellingGeneralAndAdministrativeExpenses\"]\n",
    "        fillingDate[\"otherExpenses\"] = quarter[\"otherExpenses\"]\n",
    "        fillingDate[\"operatingExpenses\"] = quarter[\"operatingExpenses\"]\n",
    "        fillingDate[\"costAndExpenses\"] = quarter[\"costAndExpenses\"]\n",
    "        fillingDate[\"interestIncome\"] = quarter[\"interestIncome\"]\n",
    "        fillingDate[\"interestExpense\"] = quarter[\"interestExpense\"]\n",
    "        fillingDate[\"depreciationAndAmortization\"] = quarter[\"depreciationAndAmortization\"]\n",
    "        fillingDate[\"ebitda\"] = quarter[\"ebitda\"]\n",
    "        fillingDate[\"operatingIncome\"] = quarter[\"operatingIncome\"]\n",
    "\n",
    "        # to the dict, we need to add a list of dicts so we have {apple:[{q1,q2,q3}]}\n",
    "        cleanIncomeStatement[ticker].append(fillingDate) \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fillingDate': '2023-11-03',\n",
       " 'calendarYear': '2023',\n",
       " 'period': 'FY',\n",
       " 'link': 'https://www.sec.gov/Archives/edgar/data/320193/000032019323000106/0000320193-23-000106-index.htm',\n",
       " 'revenue': 383285000000,\n",
       " 'costOfRevenue': 214137000000,\n",
       " 'grossProfit': 169148000000,\n",
       " 'researchAndDevelopmentExpenses': 29915000000,\n",
       " 'generalAndAdministrativeExpenses': 0,\n",
       " 'sellingAndMarketingExpenses': 0,\n",
       " 'sellingGeneralAndAdministrativeExpenses': 24932000000,\n",
       " 'otherExpenses': -565000000,\n",
       " 'operatingExpenses': 54847000000,\n",
       " 'costAndExpenses': 268984000000,\n",
       " 'interestIncome': 3750000000,\n",
       " 'interestExpense': 3933000000,\n",
       " 'depreciationAndAmortization': 11519000000,\n",
       " 'ebitda': 125820000000,\n",
       " 'operatingIncome': 114301000000}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleanIncomeStatement\n",
    "# {aaple: [{revenue, COGS, etc...}]}\n",
    "\n",
    "cleanIncomeStatement[\"aapl\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fillingDate': '2023-11-03',\n",
       " 'calendarYear': '2023',\n",
       " 'period': 'FY',\n",
       " 'link': 'https://www.sec.gov/Archives/edgar/data/320193/000032019323000106/0000320193-23-000106-index.htm',\n",
       " 'revenue': 383285000000,\n",
       " 'costOfRevenue': 214137000000,\n",
       " 'grossProfit': 169148000000,\n",
       " 'researchAndDevelopmentExpenses': 29915000000,\n",
       " 'generalAndAdministrativeExpenses': 0,\n",
       " 'sellingAndMarketingExpenses': 0,\n",
       " 'sellingGeneralAndAdministrativeExpenses': 24932000000,\n",
       " 'otherExpenses': -565000000,\n",
       " 'operatingExpenses': 54847000000,\n",
       " 'costAndExpenses': 268984000000,\n",
       " 'interestIncome': 3750000000,\n",
       " 'interestExpense': 3933000000,\n",
       " 'depreciationAndAmortization': 11519000000,\n",
       " 'ebitda': 125820000000,\n",
       " 'operatingIncome': 114301000000}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cleanIncomeStatement[\"aapl\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfList = []\n",
    "df = spark.createDataFrame([cleanIncomeStatement[\"aapl\"][0]])\n",
    "# df = spark.createDataFrame(cleanIncomeStatement[\"aapl\"][0], schema = [\"fillingDate\",\"calendarYear\",\"period\",\"link\",\"revenue\",\"costOfRevenue\",\"grossProfit\",\"researchAndDevelopmentExpenses\",\"generalAndAdministrativeExpenses\",\"sellingAndMarketingExpenses\",\"sellingGeneralAndAdministrativeExpenses\",\"otherExpenses\",\"operatingExpenses\",\"costAndExpenses\",\"interestIncome\",\"interestExpense\",\"depreciationAndAmortization\",\"ebitda\",\"operatingIncome\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o79.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (10.0.0.32 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 30 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 30 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20528\\3726558592.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o79.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (10.0.0.32 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 30 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 30 more\r\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cash Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Equity object at 0x000001FFBBC59150> + income\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
